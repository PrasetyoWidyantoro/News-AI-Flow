{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-06T09:09:52.861373Z",
     "iopub.status.busy": "2024-09-06T09:09:52.861095Z",
     "iopub.status.idle": "2024-09-06T09:09:53.222254Z",
     "shell.execute_reply": "2024-09-06T09:09:53.221253Z",
     "shell.execute_reply.started": "2024-09-06T09:09:52.861340Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T01:06:27.266468Z",
     "iopub.status.busy": "2024-09-08T01:06:27.265735Z",
     "iopub.status.idle": "2024-09-08T01:06:42.967701Z",
     "shell.execute_reply": "2024-09-08T01:06:42.966611Z",
     "shell.execute_reply.started": "2024-09-08T01:06:27.266422Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.24.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.2\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install dependencies\n",
    "!pip install transformers datasets evaluate huggingface_hub tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:55:49.082541Z",
     "iopub.status.busy": "2024-09-08T02:55:49.082083Z",
     "iopub.status.idle": "2024-09-08T02:55:49.092127Z",
     "shell.execute_reply": "2024-09-08T02:55:49.090744Z",
     "shell.execute_reply.started": "2024-09-08T02:55:49.082497Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers.data.processors.squad import SquadProcessor\n",
    "from transformers import squad_convert_examples_to_features\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:55:49.521488Z",
     "iopub.status.busy": "2024-09-08T02:55:49.520747Z",
     "iopub.status.idle": "2024-09-08T02:55:49.563360Z",
     "shell.execute_reply": "2024-09-08T02:55:49.562408Z",
     "shell.execute_reply.started": "2024-09-08T02:55:49.521448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['context', 'question', 'answers', 'id'],\n",
      "        num_rows: 57224\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['context', 'question', 'answers', 'id'],\n",
      "        num_rows: 14335\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load DatasetDict from Google Drive\n",
    "dataset_dict_path = '/kaggle/input/news-qa-data/news-qa-data'\n",
    "datasets = load_from_disk(dataset_dict_path)\n",
    "\n",
    "# Verify the content\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:55:50.103420Z",
     "iopub.status.busy": "2024-09-08T02:55:50.102381Z",
     "iopub.status.idle": "2024-09-08T02:55:51.436196Z",
     "shell.execute_reply": "2024-09-08T02:55:51.435321Z",
     "shell.execute_reply.started": "2024-09-08T02:55:50.103365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd68b0422c41474b9598b93d25157045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c128f15047944afeb6cc5cd180df8179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123019ca4fb14a9ea230f2535dd89a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080a13d1e1974f9cafab33f506e04510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:55:54.098669Z",
     "iopub.status.busy": "2024-09-08T02:55:54.098217Z",
     "iopub.status.idle": "2024-09-08T02:55:54.116164Z",
     "shell.execute_reply": "2024-09-08T02:55:54.114564Z",
     "shell.execute_reply.started": "2024-09-08T02:55:54.098626Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "stride = 128\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "\n",
    "        # Periksa jika jawaban ada\n",
    "        if len(answer[\"answer_start\"]) == 0 or len(answer[\"text\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:55:54.505070Z",
     "iopub.status.busy": "2024-09-08T02:55:54.504746Z",
     "iopub.status.idle": "2024-09-08T02:58:59.212585Z",
     "shell.execute_reply": "2024-09-08T02:58:59.211624Z",
     "shell.execute_reply.started": "2024-09-08T02:55:54.505037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505812b0ec6d4b79b60040b80e50caba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(57224, 99693)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping dataset tanpa caching\n",
    "train_dataset = datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names,\n",
    "    keep_in_memory=True \n",
    ")\n",
    "\n",
    "len(datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:58:59.214816Z",
     "iopub.status.busy": "2024-09-08T02:58:59.214430Z",
     "iopub.status.idle": "2024-09-08T02:58:59.224068Z",
     "shell.execute_reply": "2024-09-08T02:58:59.222713Z",
     "shell.execute_reply.started": "2024-09-08T02:58:59.214768Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:58:59.226737Z",
     "iopub.status.busy": "2024-09-08T02:58:59.225845Z",
     "iopub.status.idle": "2024-09-08T02:59:56.355240Z",
     "shell.execute_reply": "2024-09-08T02:59:56.354218Z",
     "shell.execute_reply.started": "2024-09-08T02:58:59.226677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586acbed5d224b9fbbc055aa46f9f91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(14335, 24856)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names,\n",
    "    keep_in_memory=True  \n",
    ")\n",
    "len(datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:59:56.358516Z",
     "iopub.status.busy": "2024-09-08T02:59:56.358095Z",
     "iopub.status.idle": "2024-09-08T02:59:56.372791Z",
     "shell.execute_reply": "2024-09-08T02:59:56.371836Z",
     "shell.execute_reply.started": "2024-09-08T02:59:56.358470Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    metric = evaluate.load(\"squad\")\n",
    "    n_best = 20  \n",
    "    max_answer_length = 200  \n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": str(example_id), \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": str(example_id), \"prediction_text\": \"\"})\n",
    "\n",
    "    # Format references to match SQuAD evaluation\n",
    "    theoretical_answers = [{\"id\": str(ex[\"id\"]), \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:59:56.374368Z",
     "iopub.status.busy": "2024-09-08T02:59:56.374009Z",
     "iopub.status.idle": "2024-09-08T02:59:58.306543Z",
     "shell.execute_reply": "2024-09-08T02:59:58.305401Z",
     "shell.execute_reply.started": "2024-09-08T02:59:56.374317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e2dd1003fc4d43a40ff96196b89536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:59:58.308469Z",
     "iopub.status.busy": "2024-09-08T02:59:58.308097Z",
     "iopub.status.idle": "2024-09-08T02:59:58.357581Z",
     "shell.execute_reply": "2024-09-08T02:59:58.356546Z",
     "shell.execute_reply.started": "2024-09-08T02:59:58.308424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-base-uncased-newsqa-squad-finetuned\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:59:58.359531Z",
     "iopub.status.busy": "2024-09-08T02:59:58.358827Z",
     "iopub.status.idle": "2024-09-08T07:44:39.144409Z",
     "shell.execute_reply": "2024-09-08T07:44:39.143231Z",
     "shell.execute_reply.started": "2024-09-08T02:59:58.359481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18693' max='18693' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18693/18693 4:44:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.754800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.777700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.751200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18693, training_loss=1.1274591832739236, metrics={'train_runtime': 17080.0529, 'train_samples_per_second': 17.51, 'train_steps_per_second': 1.094, 'total_flos': 7.814837270784614e+16, 'train_loss': 1.1274591832739236, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T07:44:39.146992Z",
     "iopub.status.busy": "2024-09-08T07:44:39.146486Z",
     "iopub.status.idle": "2024-09-08T07:55:15.089427Z",
     "shell.execute_reply": "2024-09-08T07:55:15.088241Z",
     "shell.execute_reply.started": "2024-09-08T07:44:39.146934Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8f4c8e83be48c8989a1b66b3366f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 48.83850715033136, 'f1': 63.78600315037666}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T02:27:30.155852Z",
     "iopub.status.busy": "2024-09-08T02:27:30.155398Z",
     "iopub.status.idle": "2024-09-08T02:27:30.184010Z",
     "shell.execute_reply": "2024-09-08T02:27:30.182975Z",
     "shell.execute_reply.started": "2024-09-08T02:27:30.155810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.006397548597306013,\n",
       " 'start': 47,\n",
       " 'end': 68,\n",
       " 'answer': 'Auto-sklearn and H2O.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device='cuda')\n",
    "context = \"\"\"\n",
    "AutoML: Automated Machine Learning tools like Auto-sklearn and H2O.ai simplify the process of building machine learning models.\n",
    "\"\"\"\n",
    "question = \"What are some tools for automated machine learning?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T07:55:53.546907Z",
     "iopub.status.busy": "2024-09-08T07:55:53.546089Z",
     "iopub.status.idle": "2024-09-08T07:55:53.554313Z",
     "shell.execute_reply": "2024-09-08T07:55:53.553033Z",
     "shell.execute_reply.started": "2024-09-08T07:55:53.546863Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "api = HfApi()\n",
    "token = \"______________\"\n",
    "HfFolder.save_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T07:55:57.035032Z",
     "iopub.status.busy": "2024-09-08T07:55:57.034185Z",
     "iopub.status.idle": "2024-09-08T07:56:14.219732Z",
     "shell.execute_reply": "2024-09-08T07:56:14.218401Z",
     "shell.execute_reply.started": "2024-09-08T07:55:57.034991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ee8388da514eb18afd1551e5269f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e61bedb042413a9cb23daa1440edfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Prasetyow12/bert-base-uncased-newsqa-squad-finetuned/commit/d01342d186ad8ad0f82423c248c8bc418a07dc8e', commit_message='Upload tokenizer', commit_description='', oid='d01342d186ad8ad0f82423c248c8bc418a07dc8e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"bert-base-uncased-newsqa-squad-finetuned\")\n",
    "tokenizer.push_to_hub(\"bert-base-uncased-newsqa-squad-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T07:56:33.276147Z",
     "iopub.status.busy": "2024-09-08T07:56:33.275482Z",
     "iopub.status.idle": "2024-09-08T07:56:40.473391Z",
     "shell.execute_reply": "2024-09-08T07:56:40.472270Z",
     "shell.execute_reply.started": "2024-09-08T07:56:33.276108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ffc7631f214c9b92c5e33ecb97a0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/673 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f486285a16124287a2e701dc9e67491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3d90d8d1bc4a10bb5a44ea276579ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd4c9776e6641418e105a366afe8201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae32ad7f6ad4315967f422a67e9e0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9c8163b3914f6b83cbe85da4bfb074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "# Load model dan tokenizer dari Hugging Face Hub\n",
    "model_bert_qa = AutoModelForQuestionAnswering.from_pretrained(\"Prasetyow12/bert-base-uncased-newsqa-squad-finetuned\")\n",
    "tokenizer_bert_qa = AutoTokenizer.from_pretrained(\"Prasetyow12/bert-base-uncased-newsqa-squad-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-08T07:57:09.835518Z",
     "iopub.status.busy": "2024-09-08T07:57:09.834729Z",
     "iopub.status.idle": "2024-09-08T07:57:10.547426Z",
     "shell.execute_reply": "2024-09-08T07:57:10.545965Z",
     "shell.execute_reply.started": "2024-09-08T07:57:09.835477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9155185222625732, 'start': 744, 'end': 787, 'answer': '\"Family dynamics and personal relationships'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import re\n",
    "\n",
    "# Buat pipeline untuk inference\n",
    "question_answerer = pipeline(\"question-answering\", model=model_bert_qa, tokenizer=tokenizer_bert_qa)\n",
    "\n",
    "# Contoh konteks\n",
    "context = \"\"\"\n",
    "(CNN) -- Police are investigating whether or what family issues might have prompted a California man to shoot six of his family members -- killing five -- before committing suicide. His wife was critically wounded. Authorities on Tuesday said Devan Kalathat, 42, shot his family Sunday night at his Santa Clara townhouse, killing two adults and three children. Kalathat killed his 11-year-old son, Akhil Dev; his 4-year-old daughter, Negha Dev; his 35-year-old brother-in-law Ashok Appu Poothemkandi; Poothemkandi's 25-year-old wife, Suchitra Sivaraman; and the couple's 11-month-old daughter, Ahana. Kalathat's 34-year-old wife, who was not identified, sustained multiple gunshot wounds and remains in critical condition, said Lt. Phil Cooke. \"Family dynamics and personal relationships may have played a factor,\" Cooke told reporters Tuesday. He said Kalathat was employed as an engineer and nothing indicated he was facing \"layoff or financial crisis.\" Investigators believe Kalathat used two .45-caliber semi-automatic pistols, both of which he owned. Cooke said Kalathat bought one of the pistols in February and the other nearly two weeks ago -- roughly the same time his wife's brother, Poothemkandi, arrived in California from India with Suchitra Sivaraman and Ahana. Cooke noted that Poothemkandi was an \"educated professional\" with plans to stay in the Bay Area to work on a project for a high-tech firm. Police were called after a neighbor noticed Kalathat's wounded wife outside the home around 8:30 p.m. (11:30 p.m. ET), Cooke said. When police arrived, other victims were found around the kitchen and dining room in what Cooke described as \"a very gruesome scene.\" The family shooting comes just two months after a Los Angeles father who, after he and his wife were fired from their jobs, killed her and their five young children before turning the gun on himself.\n",
    "\"\"\"\n",
    "question = \"What did police say was a factor?\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Definisikan pola regex untuk berbagai cleaning\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+', re.IGNORECASE)\n",
    "    hashtag_pattern = re.compile(r'#\\w+', re.IGNORECASE)\n",
    "    double_space_pattern = re.compile(r'\\s\\s+')\n",
    "    header_pattern = re.compile(r'^.*?--\\s?', re.IGNORECASE)\n",
    "    video_pattern = re.compile(r'VIDEO:.*?(?:\\.\\s|$)', re.IGNORECASE)\n",
    "\n",
    "    # Hapus URL\n",
    "    text = url_pattern.sub('', text)\n",
    "\n",
    "    # Hapus hashtag\n",
    "    text = hashtag_pattern.sub('', text)\n",
    "\n",
    "    # Cek jika ada '--' dalam 40 karakter pertama\n",
    "    if '--' in text[:40]:\n",
    "        # Hapus header sebelum '--'\n",
    "        text = header_pattern.sub('', text).strip()\n",
    "\n",
    "    # Hapus frasa \"VIDEO:\" hingga titik\n",
    "    text = video_pattern.sub('', text)\n",
    "\n",
    "    # Hapus double space\n",
    "    text = double_space_pattern.sub(' ', text)\n",
    "\n",
    "    # Trim leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "cleaned_context = clean_text(context)\n",
    "\n",
    "# inferencing menggunakan data yang sudah clean\n",
    "result = question_answerer(question=question, context=cleaned_context)\n",
    "\n",
    "# Output hasil inference\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family dynamics and personal relationships\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import re\n",
    "\n",
    "# Load model dan tokenizer dari Hugging Face Hub\n",
    "model_bert_qa = AutoModelForQuestionAnswering.from_pretrained(\"Prasetyow12/bert-base-uncased-newsqa-squad-finetuned\")\n",
    "tokenizer_bert_qa = AutoTokenizer.from_pretrained(\"Prasetyow12/bert-base-uncased-newsqa-squad-finetuned\")\n",
    "\n",
    "# Buat pipeline untuk inference\n",
    "question_answerer = pipeline(\"question-answering\", model=model_bert_qa, tokenizer=tokenizer_bert_qa)\n",
    "\n",
    "# Contoh konteks\n",
    "context = \"\"\"\n",
    "(CNN) -- Police are investigating whether or what family issues might have prompted a California man to shoot six of his family members -- killing five -- before committing suicide. His wife was critically wounded. Authorities on Tuesday said Devan Kalathat, 42, shot his family Sunday night at his Santa Clara townhouse, killing two adults and three children. Kalathat killed his 11-year-old son, Akhil Dev; his 4-year-old daughter, Negha Dev; his 35-year-old brother-in-law Ashok Appu Poothemkandi; Poothemkandi's 25-year-old wife, Suchitra Sivaraman; and the couple's 11-month-old daughter, Ahana. Kalathat's 34-year-old wife, who was not identified, sustained multiple gunshot wounds and remains in critical condition, said Lt. Phil Cooke. \"Family dynamics and personal relationships may have played a factor,\" Cooke told reporters Tuesday. He said Kalathat was employed as an engineer and nothing indicated he was facing \"layoff or financial crisis.\" Investigators believe Kalathat used two .45-caliber semi-automatic pistols, both of which he owned. Cooke said Kalathat bought one of the pistols in February and the other nearly two weeks ago -- roughly the same time his wife's brother, Poothemkandi, arrived in California from India with Suchitra Sivaraman and Ahana. Cooke noted that Poothemkandi was an \"educated professional\" with plans to stay in the Bay Area to work on a project for a high-tech firm. Police were called after a neighbor noticed Kalathat's wounded wife outside the home around 8:30 p.m. (11:30 p.m. ET), Cooke said. When police arrived, other victims were found around the kitchen and dining room in what Cooke described as \"a very gruesome scene.\" The family shooting comes just two months after a Los Angeles father who, after he and his wife were fired from their jobs, killed her and their five young children before turning the gun on himself.\n",
    "\"\"\"\n",
    "question = \"What did police say was a factor?\"\n",
    "\n",
    "# Fungsi untuk cleaning teks\n",
    "def clean_text(text):\n",
    "    # Definisikan pola regex untuk berbagai cleaning\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+', re.IGNORECASE)\n",
    "    hashtag_pattern = re.compile(r'#\\w+', re.IGNORECASE)\n",
    "    double_space_pattern = re.compile(r'\\s\\s+')\n",
    "    header_pattern = re.compile(r'^.*?--\\s?', re.IGNORECASE)\n",
    "    video_pattern = re.compile(r'VIDEO:.*?(?:\\.\\s|$)', re.IGNORECASE)\n",
    "\n",
    "    # Hapus URL\n",
    "    text = url_pattern.sub('', text)\n",
    "\n",
    "    # Hapus hashtag\n",
    "    text = hashtag_pattern.sub('', text)\n",
    "\n",
    "    # Cek jika ada '--' dalam 40 karakter pertama\n",
    "    if '--' in text[:40]:\n",
    "        # Hapus header sebelum '--'\n",
    "        text = header_pattern.sub('', text).strip()\n",
    "\n",
    "    # Hapus frasa \"VIDEO:\" hingga titik\n",
    "    text = video_pattern.sub('', text)\n",
    "\n",
    "    # Hapus double space\n",
    "    text = double_space_pattern.sub(' ', text)\n",
    "\n",
    "    # Trim leading and trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Gunakan clean_text untuk cleaning konteks\n",
    "cleaned_context = clean_text(context)\n",
    "\n",
    "# Lakukan inference menggunakan konteks yang sudah dicleaning\n",
    "result = question_answerer(question=question, context=cleaned_context)\n",
    "\n",
    "# Output hasil inference hanya bagian jawabannya, tanpa tanda kutip\n",
    "cleaned_answer = result['answer'].strip('\"')\n",
    "\n",
    "# Cetak jawaban yang sudah dibersihkan\n",
    "print(cleaned_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5661278,
     "sourceId": 9341526,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
