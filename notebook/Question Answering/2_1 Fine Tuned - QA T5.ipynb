{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ab00e-19a4-4880-8998-4cd6ce6f8e58",
   "metadata": {
    "id": "aa3ab00e-19a4-4880-8998-4cd6ce6f8e58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, GenerationConfig, pipeline\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9deefd-5e23-40d1-83c8-aa5a2d95c83f",
   "metadata": {
    "id": "da9deefd-5e23-40d1-83c8-aa5a2d95c83f",
    "outputId": "37c1a1b2-ae2e-4ddc-e0c2-a86c1b61a942",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ad1ce-d472-48ad-b7dd-44ba08cd7f39",
   "metadata": {
    "id": "553ad1ce-d472-48ad-b7dd-44ba08cd7f39",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"sample1k_train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"sample1k_val.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(\"sample1k_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d56a7b-71b0-4465-a237-22e24f23fe6b",
   "metadata": {
    "id": "94d56a7b-71b0-4465-a237-22e24f23fe6b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_data)\n",
    "df_val = pd.DataFrame(val_data)\n",
    "df_test = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c591d98-7771-41fa-8a80-b32ab0844a78",
   "metadata": {
    "id": "6c591d98-7771-41fa-8a80-b32ab0844a78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def context_template(df):\n",
    "    df[\"context\"] = f\"\"\"Context:\n",
    "{df[\"text\"]}\n",
    "\n",
    "Question:\n",
    "{df[\"questions\"]}\"\"\"\n",
    "    return df\n",
    "\n",
    "df_train = df_train.apply(context_template, axis=1)\n",
    "df_val = df_val.apply(context_template, axis=1)\n",
    "df_test = df_test.apply(context_template, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9de596a-f26b-4c81-9997-59723f4b5d40",
   "metadata": {
    "id": "a9de596a-f26b-4c81-9997-59723f4b5d40",
    "outputId": "8572e7cf-9f5f-4496-c77b-21d6bf0dac66",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4131, 4), (885, 4), (886, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd8189-e26e-4643-b5a5-56fa6c4ad4f9",
   "metadata": {
    "id": "09fd8189-e26e-4643-b5a5-56fa6c4ad4f9",
    "outputId": "b2a9416e-0f44-488a-8f70-d7cc94717ef0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.duplicated().sum(), df_val.duplicated().sum(), df_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5f641-4802-4681-be9d-596f2227a8fa",
   "metadata": {
    "id": "f8a5f641-4802-4681-be9d-596f2227a8fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "valid_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': valid_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7423f-f73b-4849-b3bd-291a988a7297",
   "metadata": {
    "id": "adb7423f-f73b-4849-b3bd-291a988a7297"
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6652d7-6de8-47cb-bb02-e55cd35ad5a6",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d8f223cc3f5c45c095cf6e3ba2b61e19",
      "f1b5897b6559441f981aa1dd210c267f",
      "13ef7917e1d44943a262ff053f512126",
      "4ed0c781a792483f9dbc16be7ff86594",
      "4dc6237bada849c1b220cd74ffb20750",
      "670fd5faf80640598933fd962d3a824b",
      "5e59f35efbb24d70ab84b915b79c16a4"
     ]
    },
    "id": "5b6652d7-6de8-47cb-bb02-e55cd35ad5a6",
    "outputId": "2cae450e-5798-40d1-98cb-b36fc6364d66",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f223cc3f5c45c095cf6e3ba2b61e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andre\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b5897b6559441f981aa1dd210c267f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ef7917e1d44943a262ff053f512126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed0c781a792483f9dbc16be7ff86594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc6237bada849c1b220cd74ffb20750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670fd5faf80640598933fd962d3a824b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e59f35efbb24d70ab84b915b79c16a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c1451-9022-4258-ba73-19fe2c28d0a9",
   "metadata": {
    "id": "384c1451-9022-4258-ba73-19fe2c28d0a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"context\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"answers\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef90efe-cd09-4037-9b27-5a50710ddd67",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "0bb35a70d3d242e39cc95b3111ec491a",
      "bc842a5ec69f41cb8e98fe4166410e8d",
      "8d5811e0c849454fbc01f5d4c081f1a2"
     ]
    },
    "id": "aef90efe-cd09-4037-9b27-5a50710ddd67",
    "outputId": "9f7ee109-823c-4e74-b4b3-409a2a6dda7d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb35a70d3d242e39cc95b3111ec491a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc842a5ec69f41cb8e98fe4166410e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/885 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5811e0c849454fbc01f5d4c081f1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd183e6-d791-4261-ac4c-71ad1d71e6cf",
   "metadata": {
    "id": "4fd183e6-d791-4261-ac4c-71ad1d71e6cf"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82ca9c-5c25-4b7e-bfcf-01f5174638e2",
   "metadata": {
    "id": "9c82ca9c-5c25-4b7e-bfcf-01f5174638e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.03,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    # logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"exact_match\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf5da5-b48f-439a-b965-3ba5d5a7d924",
   "metadata": {
    "id": "afbf5da5-b48f-439a-b965-3ba5d5a7d924",
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load('exact_match')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # Directly decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # decoded_preds = [\" \".join(decoded_pred.split()) for decoded_pred in decoded_preds]\n",
    "    # decoded_labels = [\" \".join(decoded_label.split()) for decoded_label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4e9bc-7b66-4e19-af18-05875fa57ad9",
   "metadata": {
    "id": "4fc4e9bc-7b66-4e19-af18-05875fa57ad9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bf38b-e0b2-431a-8207-387a0088d094",
   "metadata": {
    "id": "ad0bf38b-e0b2-431a-8207-387a0088d094",
    "outputId": "17bdd786-437e-4450-ba4d-2358226f82b0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\python310\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='222' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 04:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3595364093780518,\n",
       " 'eval_exact_match': 0.05191873589164785,\n",
       " 'eval_runtime': 41.5051,\n",
       " 'eval_samples_per_second': 21.347,\n",
       " 'eval_steps_per_second': 2.674}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4356684-fa44-4d47-ab38-ae181cff4ee3",
   "metadata": {
    "id": "a4356684-fa44-4d47-ab38-ae181cff4ee3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369259b-d286-47cf-b30a-7a23a28fb01f",
   "metadata": {
    "id": "2369259b-d286-47cf-b30a-7a23a28fb01f",
    "outputId": "abd2ad7a-a984-499b-cd63-46344cf9785f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1290' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1290/1290 18:14, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.480700</td>\n",
       "      <td>1.245798</td>\n",
       "      <td>0.131073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.261200</td>\n",
       "      <td>1.229409</td>\n",
       "      <td>0.132203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.214800</td>\n",
       "      <td>1.227856</td>\n",
       "      <td>0.132203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\python310\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\andre\\python310\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\andre\\python310\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\andre\\python310\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1290, training_loss=1.3401988125586695, metrics={'train_runtime': 1095.2883, 'train_samples_per_second': 18.858, 'train_steps_per_second': 1.178, 'total_flos': 3833062206996480.0, 'train_loss': 1.3401988125586695, 'epoch': 4.990328820116054})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d30894-d8a9-427d-9c7b-a29d1fd04890",
   "metadata": {
    "id": "24d30894-d8a9-427d-9c7b-a29d1fd04890",
    "outputId": "3abf916d-7b51-43ce-de24-277a0d4c8341",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\python310\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='111' max='111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [111/111 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3321141004562378, 'eval_exact_match': 0.12641083521444696, 'eval_runtime': 38.1251, 'eval_samples_per_second': 23.239, 'eval_steps_per_second': 2.911, 'epoch': 4.990328820116054}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63f1e7-a9ff-451e-b262-b0e33c1e1d15",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "00093f9623ec4dd7a8de97c5ff10c1be",
      "e2ec4d04b7324fa89f676657893fa865",
      "91f7a9a2238e4753bb6f603106b11500"
     ]
    },
    "id": "ff63f1e7-a9ff-451e-b262-b0e33c1e1d15",
    "outputId": "afe28d8c-866f-487f-a28a-0b3d8d3bbc16",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00093f9623ec4dd7a8de97c5ff10c1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ec4d04b7324fa89f676657893fa865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\python310\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\andre\\.cache\\huggingface\\hub\\models--andreanstev--t5_news_qa. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f7a9a2238e4753bb6f603106b11500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/andreanstev/t5_news_qa/commit/519ac0549be79da60b5019ed71fd67da8fe36412', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='519ac0549be79da60b5019ed71fd67da8fe36412', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"t5_news_qa\", token=\"------------\")\n",
    "model.push_to_hub(\"t5_news_qa\", token=\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a8f06-bed7-4701-8464-39fd3d603417",
   "metadata": {
    "id": "6a2a8f06-bed7-4701-8464-39fd3d603417"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d45c9-b48a-4635-8b65-0f178ef2805d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "98d21b2055224a29aaedc8e76276b507",
      "5a13821cdd894b2b8686ad0ea28533e8",
      "6a50276005a3411bbae9cb3c9badfe1d",
      "8f32efa268b2480db0f024f18db1c368",
      "3559e091179c4e018e40429166e9d24b",
      "c250e643818e4320bea351c288771cbd",
      "368278324d9a479a96b7ddcd1051e00f"
     ]
    },
    "id": "2c8d45c9-b48a-4635-8b65-0f178ef2805d",
    "outputId": "d5382a26-8347-41c7-d8b5-577b05c42224",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d21b2055224a29aaedc8e76276b507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a13821cdd894b2b8686ad0ea28533e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a50276005a3411bbae9cb3c9badfe1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/118 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f32efa268b2480db0f024f18db1c368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/21.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3559e091179c4e018e40429166e9d24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c250e643818e4320bea351c288771cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368278324d9a479a96b7ddcd1051e00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline(\"text2text-generation\", model=\"andreanstev/t5_news_qa\", device ='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fea0d8-c9fa-4f28-bcb8-28aded5d1bbb",
   "metadata": {
    "id": "b8fea0d8-c9fa-4f28-bcb8-28aded5d1bbb",
    "outputId": "8eb6c35e-4e98-404b-f496-1173ce03801b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (904 > 512). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\andre\\python310\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Helmand province'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE = \"\"\"\\'SINDH KALAY\\', England (CNN) -- The aroma of freshly baking flatbread wafts through the air as a unit of British soldiers position themselves for a quick patrol around the village of Sindh Kalay. A British soldier on patrol in the mock Afghan village of Sindh Kalay. Market vendors hawk grapes and melons, as a group of village elders sit smoking water pipes and suspicious-looking men lurk beside battered motorcycles. What should the soldiers do? Conduct a weapons search? Approach the village elders first? In the complex political and cultural terrain of Afghanistan, what is the best course of action? Except this is not Afghanistan. It\\'s Norfolk, England. Instead of the Hindu Kush mountains, it is the green ladscape and tidy farmhouses of the English countryside that stretch out behind them. Welcome to the British Army\\'s state-of-the art training ground. It cost more than $20 million to build and every British soldier serving in Afghanistan will do his or her training here. \"I think it\\'s the closest thing you are going to get short of being in Afghanistan itself,\" says Col. David Colthup of the 2nd Battalion of the Yorkshire Regiment. His troops have already served one tour of duty in Afghanistan\\'s Helmand province and are training for another. British troops serving in Helmand province are tasked with mentoring and training Afghan security forces. Not an easy job in a Taliban stronghold and Afghanistan\\'s center of opium production. \"Ultimately, a soldier joins the army and trains to fight. That\\'s what a soldier trains to do. But today, it\\'s a much, much more complex environment,\" explains Colthup. \"The business of being able to interact either through an interpreter or through Afghan security forces, whether they are police or army. And to understand how the people operate and how we can interact better with them. Because ultimately, that\\'s what it\\'s about,\" he says. The most distinctive features of Sindh Kalay are the high three-meter walls that make up the village compound, creating narrow alleyways difficult for troops to patrol. The village is staffed with Afghan asylum-seekers, many of whom have fled the Taliban. They play the roles of market vendors, village elders and sometimes Afghan security forces. Several Afghan women are also on hand, useful for training British soldiers on the religious and cultural sensitivities of entering an Afghan home.  Watch British troops training in mock Afghan village » The Taliban insurgents are played by Nepalese Ghurkha soldiers authorized to handle weapons. They play their roles silently, unable to partake in the Pashtun banter among the Afghans. Fazel Beria is also an asylum-seeker from Afghanistan. He is responsible for recruiting and for creating the sights and smells of Sindh Kalay and is easily identifiable as the only Afghan in the market in Western clothes. He beams with pride walking down the bazaar and clearly relishes his role in training the British Army. \"Everything with the culture comes up with the issue of hearts and minds,\" he explains. \"If you want to win that, you need to know about their culture. You need to respect their culture, their religion and their way of life.\" He gives high marks to the soldiers training so far. After each exercise, the Afghan actors talk directly to the soldiers about what went wrong and what went right. Sometimes, it\\'s the little things that count. \"Yes, there have been quite a lot of surprises,\" Beria says. Like Afghan will sit cross legged for hours. \"The British soldier cannot do that,\" he laughs. \"The Afghan will be sitting very comfortable and the British soldier is not. So, they have to get used to it.\"  See photos of British troops on patrol in Sindh Kalay -- and for real in Afghanistan » Previously, the army trained on farmhouses and in urban neighborhoods that resembled Northern Ireland more than Afghanistan. But Sindh Kalay does more than mimic the physical reality of Afghanistan. It also mirrors the changing tactics on the ground. Troops are grilled in\"\"\"\n",
    "question = \"Where will British troops be deployed?\"\n",
    "input = f\"\"\"Context:\n",
    "{ARTICLE}\n",
    "\n",
    "Question:\n",
    "{question}\"\"\"\n",
    "res = qa(input)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
